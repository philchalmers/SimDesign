%\VignetteIndexEntry{SimDesign}
%\VignetteEngine{knitr::knitr}
---
title: "SimDesign"
author: "Phil Chalmers"
date: "September 22, 2015"
output:
  html_document:
    number_sections: yes
    toc: yes
---

```{r include=FALSE}
options(digits = 3)
```

# Simulation: Comparing two estimators and how effectively they recover population parameters

Item factor analysis (IFA) is a common method for determine the effectiveness and structure of items in
psychological tests. Two approaches currently exist: full-information IFA via method from the item response
theory framework, and limited-information IFA from the structural equation modeling framework. The question is,
under which circumstances should either method be used? Here we explore a simple one factor test structure
to determine which approach can recover the item parameters when varying test length and sample size.

Unforunatly, the `mirt` package uses a slightly different parameterization of the IFA problem, implementing 
a logistic model rather than a normal ogive model. Historically, the response curves have been equated 
by applying a scaling correction of $D = 1.702$ to help fix this issue, which as can be seen below makes the 
response curves very similar (as it turns out, logistic models have slightly thicker tails than ogive models).

To give the following simulation the benifit of the doubt, the data will be 
generated from the normal ogive model (implying that `lavaan` is fitting the correct model) while `mirt`
is only fitting an approximation to this model by translating the parameters. From this we will see whether
FIML with a logistic model will better approximate the normal ogive parmaters compared to the limited information
DWLS approach available in `lavaan`.

```{r}
P_logit <- function(a, d, Theta) exp(a * Theta + d) / (1 + exp(a * Theta + d))
P_ogive <- function(a, d, Theta) pnorm(a * Theta + d)

Theta <- seq(-5,5,length.out=200)
a <- 0.5
d <- -.5
D <- 1.702

example <- data.frame(Theta, logit=P_logit(a*D, d*D, Theta), ogive=P_ogive(a, d, Theta))
plot(Theta, example$logit, type = 'l', ylab = 'P', las=1)
lines(Theta, example$ogive, col = 'red')
```


### Define the conditions

Start by defining the conditions to be studied. Because we are interested in a number of slopes and intercepts
it is often convenient to save long strings of numbers to external files and read them in during the simulation.
Naturally, these could be included in the simulation source code directly, but often will take up a large amount
of space (could be hundreds of lines). Because these are fixed and known a prior, writing these to an external
file is a good strategy.

```{r}
sample_sizes <- c(100, 250, 500, 1000)
test_length <- c(10, 30)
Design <- expand.grid(sample_size=sample_sizes, test_length=test_length)

# save extra verbose information in an external file, and read it in later
set.seed(1234)
aux_info <- vector('list', 2)
names(aux_info) <- test_length
for(i in 1:2)
    aux_info[[i]] <- list(a = round(rlnorm(test_length[i], .2, .3)/1.702, 2), 
                          d = round(rnorm(test_length[i], 0, .5)/1.702, 2))
aux_info
saveRDS(aux_info, 'parameters.rds')
```

### Define the functions

As usual, define the functions of interest. Here we make sure that `lavaan` and `mirt` are loaded in when they
are required by calling a suitable `library()` call to make their functions available (required when 
running simulations in parallel).

```{r}
library(SimDesign)
# SimDesign_functions()
```

```{r}
Generate <- function(condition) {
    
    P_ogive <- function(a, d, Theta) pnorm(a*Theta + d)
    
    nitems <- condition$test_length
    N <- condition$sample_size
        
    #source in for convience (otherwise, these could be manually define in the source code)
    parameters <- readRDS('parameters.rds')
    a <- matrix(parameters[[as.character(nitems)]]$a)
    d <- matrix(parameters[[as.character(nitems)]]$d)
    
    dat <- matrix(NA, N, nitems)
    colnames(dat) <- paste0('item_', 1:nitems)
    Theta <- rnorm(N)
    for(j in 1:nitems){
        p <- P_ogive(a[j], d[j], Theta)
        for(i in 1:N)
            dat[i,j] <- sample(c(1,0), 1, prob = c(p[i], 1 - p[i])) 
    }
    return(as.data.frame(dat))
}

Analyse <- function(condition, dat, parameters = NULL) {
    
    library(mirt)
    library(lavaan)
    nitems <- condition$test_length
    
    mod <- try(mirt(dat, 1L, verbose=FALSE), silent=TRUE)
    check_error(mod)
    if(mod@converge != 1) stop('did not converge')
    cfs <- coef(mod, simplify = TRUE, digits = Inf)
    FIML_as <- cfs$items[,1L] / 1.702
    FIML_ds <- cfs$items[,2L] / 1.702
    
    lavmod <- paste0('F =~ ', paste0('NA*', colnames(dat)[1L], ' + '), 
                     paste0(colnames(dat)[-1L], collapse = ' + '),
                     '\nF ~~ 1*F')
    lmod <- try(sem(lavmod, dat, ordered = colnames(dat)))
    check_error(lmod)
    if(!lmod@Fit@converged) stop('did not converge')
    cfs2 <- coef(lmod) 
    DWLS_as <- cfs2[1L:nitems]
    DWLS_ds <- -1*cfs2[(1L:nitems) + nitems]

    return(c(FIML_as=unname(FIML_as), FIML_ds=unname(FIML_ds), 
             DWLS_as=unname(DWLS_as), DWLS_ds=unname(DWLS_ds)))
}

Summarise <- function(condition, results, parameters_list = NULL) {
    
    parameters <- readRDS('parameters.rds')
    nitems <- as.character(condition$test_length)
    pop_as <- matrix(parameters[[nitems]]$a)
    pop_ds <- matrix(parameters[[nitems]]$d)
    pop <- c(pop_as, pop_ds, pop_as, pop_ds)
    
    index <- 1:ncol(results)
    
    obt_bias <- sapply(index, function(ind, obs, pop) bias(obs[,ind], pop[ind]),
                       obs = results, pop = pop)
    obt_RMSE <- sapply(index, function(ind, obs, pop) RMSE(obs[,ind], pop[ind]),
                       obs = results, pop = pop)
    names(obt_bias) <- names(obt_RMSE) <- colnames(results)
    
    ret <- c(bias=obt_bias, RMSE=obt_RMSE)
    return(ret)
}
```

### Run the simulation

Because this simulation takes considerably longer it is recommended to pass the `save = TRUE` to temporarily
save results in case of power outages. Results can be continued by running the identical simulation code as the
initial run, and the function will automatically detect whether any temp files are available and resume the 
simulation at the previously saved location.

```{r cache=TRUE}
res10 <- runSimulation(subset(Design, test_length == 10), replications = 1000, verbose = FALSE, save = TRUE,
                       generate=Generate, analyse=Analyse, summarise=Summarise, parallel = TRUE)
```

```{r cache=TRUE}
res30 <- runSimulation(subset(Design, test_length == 30), replications = 1000, verbose = FALSE, save = TRUE,
                       generate=Generate, analyse=Analyse, summarise=Summarise, parallel = TRUE)
```

### Analyze the results

Sometimes, reshaping and indexing your output can be very helpful. Here we break the analysis into two parts,
though other strategies are certainly possible. Because analyzing simulations is a lot like analyzing empirical
data no one strategy may be the best; you have to use judgment.

#### Ten items

```{r}
# bias in slopes
names10 <- colnames(res10)
bias_as_fiml <- t(res10[,grepl('bias\\.', names10) & grepl('\\_as', names10) & 
                       grepl('FIML', names10)])
colnames(bias_as_fiml) <- sample_sizes
rownames(bias_as_fiml) <- aux_info[["10"]]$a

bias_as_dwls <- t(res10[,grepl('bias\\.', names10) & grepl('\\_as', names10) & 
                       grepl('DWLS', names10)])
colnames(bias_as_dwls) <- sample_sizes
rownames(bias_as_dwls) <- aux_info[["10"]]$a

(out <- list(FIML=bias_as_fiml, DWLS=bias_as_dwls))
sapply(out, colMeans)

# RMSE in slopes
RMSE_as_fiml <- t(res10[,grepl('RMSE\\.', names10) & grepl('\\_as', names10) & 
                       grepl('FIML', names10)])
colnames(RMSE_as_fiml) <- sample_sizes
rownames(RMSE_as_fiml) <- aux_info[["10"]]$a

RMSE_as_dwls <- t(res10[,grepl('RMSE\\.', names10) & grepl('\\_as', names10) & 
                       grepl('DWLS', names10)])
colnames(RMSE_as_dwls) <- sample_sizes
rownames(RMSE_as_dwls) <- aux_info[["10"]]$a

(out <- list(FIML=RMSE_as_fiml, DWLS=RMSE_as_dwls))
sapply(out, colMeans)
```

Regarding the slope parameters, DWLS appears to be fairly biased across different slope values
resulting in a systematic under-estimated by a large amount (as indicated by the large negative bias). 
Furthermore, the bias DWLS did not appear to improve as sample size increased, though overall the RMSE did
slightly improve in larger samples. 

FIML, however, did perform better as the sample size increased, and did not seem to be systematically 
biased. Only when $N=100$ did it appear that DWLS appear to outperform FIML in terms of RMSE, but given that 
DWLS is systematically biased downwords it's difficult to claim that this approach should be recommended
even in very small sample sizes.

Additionally, there appears to be an effect relating to the size of the parameters. Regarding DWLS, 
larger slopes tend to be recovered with more bias than slopes which are closer to 0. Unfortunately,
as bias increase so too does RMSE, therefore DWLS appears to be negatively effected as the slopes become
larger in magnitude. Both estimators appeared to have more sampling variability for larger slopes (see
the RMSE plot below), but the DWLS appeared to perform much worse than the FIML estimator for nearly all
parameters.

```{r}
library(ggplot2)
plt <- data.frame(pars = c(aux_info[["10"]]$a, aux_info[["10"]]$a), 
                  RMSE = c(RMSE_as_fiml[,'1000'], RMSE_as_dwls[,'1000']),
                  bias = c(bias_as_fiml[,'1000'], bias_as_dwls[,'1000']),
                  estimator = rep(c('FIML', 'DWLS'), each = 10))
ggplot(plt, aes(pars, bias, colour=estimator)) + geom_point(size=2) + facet_wrap(~estimator) + 
    ggtitle('slope sizes by bias for FIML and DWLS estimators')
ggplot(plt, aes(pars, RMSE, colour=estimator)) + geom_point(size=2) + facet_wrap(~estimator) + 
    ggtitle('slope sizes by RMSE for FIML and DWLS estimators')

```

```{r}
# bias in intercepts
bias_ds_fiml <- t(res10[,grepl('bias\\.', names10) & grepl('\\_ds', names10) & 
                       grepl('FIML', names10)])
colnames(bias_ds_fiml) <- sample_sizes
rownames(bias_ds_fiml) <- aux_info[["10"]]$d

bias_ds_dwls <- t(res10[,grepl('bias\\.', names10) & grepl('\\_ds', names10) & 
                       grepl('DWLS', names10)])
colnames(bias_ds_dwls) <- sample_sizes
rownames(bias_ds_dwls) <- aux_info[["10"]]$d

(out <- list(FIML=bias_ds_fiml, DWLS=bias_ds_dwls))
sapply(out, colMeans)

# RMSE in intercepts
RMSE_ds_fiml <- t(res10[,grepl('RMSE\\.', names10) & grepl('\\_ds', names10) & 
                       grepl('FIML', names10)])
colnames(RMSE_ds_fiml) <- sample_sizes
rownames(RMSE_ds_fiml) <- aux_info[["10"]]$d

RMSE_ds_dwls <- t(res10[,grepl('RMSE\\.', names10) & grepl('\\_ds', names10) & 
                       grepl('DWLS', names10)])
colnames(RMSE_ds_dwls) <- sample_sizes
rownames(RMSE_ds_dwls) <- aux_info[["10"]]$d

(out <- list(FIML=RMSE_ds_fiml, DWLS=RMSE_ds_dwls))
sapply(out, colMeans)
```

Moving now to the intercepts, both DWLS and FIML appeared to become less biased as the sample size increased,
however FIML was overall slightly less biased than DWLS. The two estimators also appeared fairly comperable
in recovering the intercept parameters, where DWLS performed slightly better in smaller sample sizes than FIML.
Both estimators also become more accurate as $N$ increased.

#### Thirty items

```{r}
# bias in slopes
names30 <- colnames(res30)
bias_as_fiml <- t(res30[,grepl('bias\\.', names30) & grepl('\\_as', names30) & 
                       grepl('FIML', names30)])
colnames(bias_as_fiml) <- sample_sizes
rownames(bias_as_fiml) <- aux_info[["30"]]$a

bias_as_dwls <- t(res30[,grepl('bias\\.', names30) & grepl('\\_as', names30) & 
                       grepl('DWLS', names30)])
colnames(bias_as_dwls) <- sample_sizes
rownames(bias_as_dwls) <- aux_info[["30"]]$a

(out <- list(FIML=bias_as_fiml, DWLS=bias_as_dwls))
sapply(out, colMeans)

# RMSE in slopes
RMSE_as_fiml <- t(res30[,grepl('RMSE\\.', names30) & grepl('\\_as', names30) & 
                       grepl('FIML', names30)])
colnames(RMSE_as_fiml) <- sample_sizes
rownames(RMSE_as_fiml) <- aux_info[["30"]]$a

RMSE_as_dwls <- t(res30[,grepl('RMSE\\.', names30) & grepl('\\_as', names30) & 
                       grepl('DWLS', names30)])
colnames(RMSE_as_dwls) <- sample_sizes
rownames(RMSE_as_dwls) <- aux_info[["30"]]$a

(out <- list(FIML=RMSE_as_fiml, DWLS=RMSE_as_dwls))
sapply(out, colMeans)
```

```{r}
# bias in intercepts
bias_ds_fiml <- t(res30[,grepl('bias\\.', names30) & grepl('\\_ds', names30) & 
                       grepl('FIML', names30)])
colnames(bias_ds_fiml) <- sample_sizes
rownames(bias_ds_fiml) <- aux_info[["30"]]$d

bias_ds_dwls <- t(res30[,grepl('bias\\.', names30) & grepl('\\_ds', names30) & 
                       grepl('DWLS', names30)])
colnames(bias_ds_dwls) <- sample_sizes
rownames(bias_ds_dwls) <- aux_info[["30"]]$d

(out <- list(FIML=bias_ds_fiml, DWLS=bias_ds_dwls))
sapply(out, colMeans)

# RMSE in intercepts
RMSE_ds_fiml <- t(res30[,grepl('RMSE\\.', names30) & grepl('\\_ds', names30) & 
                       grepl('FIML', names30)])
colnames(RMSE_ds_fiml) <- sample_sizes
rownames(RMSE_ds_fiml) <- aux_info[["30"]]$d

RMSE_ds_dwls <- t(res30[,grepl('RMSE\\.', names30) & grepl('\\_ds', names30) & 
                       grepl('DWLS', names30)])
colnames(RMSE_ds_dwls) <- sample_sizes
rownames(RMSE_ds_dwls) <- aux_info[["30"]]$d

(out <- list(FIML=RMSE_ds_fiml, DWLS=RMSE_ds_dwls))
sapply(out, colMeans)
```

The same trend appeared in the 30 item simulation case, where again FIML was less biased in both intercepts and 
slopes than DWLS, DWLS was systmetically biased in the slope parameters, and 
DWLS performed slightly better in the $N=100$ condition for the slope parameters. 

```{r include=FALSE}
system('rm *.rds')
```

