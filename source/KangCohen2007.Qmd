%\VignetteIndexEntry{SimDesign}
%\VignetteEngine{knitr::knitr}
---
title: "Kang and Cohen (2007) simulation 2"
author: "Phil Chalmers"
output:
  html_document:
    number_sections: yes
    toc: yes
    code_download: true
---

```{r include=FALSE}
options(digits = 3)
cache_ <- TRUE
```

Simulation from: 

- Kang T, Cohen AS. IRT Model Selection Methods for Dichotomous Items. Applied Psychological Measurement. 2007;31(4):331-358. doi:10.1177/0146621606292213
- https://journals.sagepub.com/doi/10.1177/0146621606292213

However, this simulation is only a partial one as I was mainly interested in replicating 
the IRT estimation results that utilized modal estimation rather than a complete MCMC
form. Plus, the article reports the DIC information criteria for the Bayesian models, which 
these days is generally regarded as sub-optimal. Moreover, computation of a partial Bayes factor
could have been replaced with a Bayes factor computation via bridge-sampling methods. 

# Simulation code

The following code uses *TODO* placeholders in case users are interested in investigating
the MCMC results themselves in the context of this code-base.

```{r include=FALSE}
library(SimDesign)
```

```{r eval=FALSE}
# SimDesign::SimFunctions(nAnalyse=2)
library(SimDesign)

Design <- createDesign(nitems = c(20,40),
                       ability_mean = c(0,-1,1),
                       sample_size = c(500,1000),
                       model = c('1PL','2PL','3PL'),
                       estimator=c('MML')) # add 'MCMC' when supported
Design

# source in helper functions + fixed_objects definition
source("KangCohen2007_extras.R")

#-------------------------------------------------------------------

Generate <- function(condition, fixed_objects = NULL) {
    Attach(condition)
    
    # get population generating parameters
    theta <- matrix(rnorm(sample_size, mean=ability_mean))
    pars <- get_parameters(condition, fixed_objects)
    
    # simulate response data (via mirt)
    dat <- with(pars, 
                simdata(sample_size, a=a, d=d, guess=c, 
                        Theta = theta, itemtype = '3PL'))
    dat
}

Analyse.MML <- function(condition, dat, fixed_objects = NULL) {
    Attach(condition)
    AnalyseIf(estimator == "MML")
    
    # MML estimation
    fit <- sprintf("Theta = 1-%i
                   START = (1-%i, a1, 1.0)
                   FIXED = (1-%i, a1)", nitems, nitems, nitems)
    mod1PL <- mirt(dat, model=fit, itemtype='2PL', verbose = FALSE)
    mod2PL <- mirt(dat, model=1, itemtype='2PL', verbose = FALSE)
    mod3PL <- mirt(dat, model=1, itemtype='3PL', verbose = FALSE)
      
    stopifnot(extract.mirt(mod1PL, 'converged'))
    stopifnot(extract.mirt(mod2PL, 'converged'))
    stopifnot(extract.mirt(mod3PL, 'converged'))
    
    # just return model comparison information (main purpose of the paper)
    ret <- as.data.frame(c(M1PL=anova(mod1PL)[c('AIC', 'BIC')], 
                           M2PL=anova(mod2PL)[c('AIC', 'BIC')],
                           M3PL=anova(mod3PL)[c('AIC', 'BIC')],
                           LR12=anova(mod1PL, mod2PL, verbose=FALSE)[2, c('X2', 'p')],
                           LR23=anova(mod2PL, mod3PL, verbose=FALSE)[2, c('X2', 'p')]))
    ret
}

Analyse.MCMC <- function(condition, dat, fixed_objects = NULL) {
    Attach(condition)
    AnalyseIf(estimator == "MCMC")
    
    # TODO
    
}

Summarise <- function(condition, results, fixed_objects = NULL) {
    nms <- colnames(results)
    means <- colMeans(results[,!grepl(".p\\b", nms)])
    sds <- apply(results[,!grepl(".p\\b", nms)], 2, sd)
    BIC <- best_info(results[,grepl(".BIC", nms)])
    ret <- c(means=means, sds=sds, BIC)
    if(condition$estimator == 'MML'){
        AIC <- best_info(results[,grepl(".AIC\\b", nms)])
        ps <- results[,grepl(".p\\b", nms)] < .05
        ps[ps[,2], 1] <- FALSE     ## if 3PLM test sig, 1PL vs 2PL doesn't matter
        LR <- colMeans(cbind(ifelse(rowSums(ps) == 0, TRUE, FALSE), ps))
        names(LR) <- c('M1PL.LR', 'M2PL.LR', 'M3PL.LR')
        ret <- c(ret, AIC, LR)
    }
    ret
}

#-------------------------------------------------------------------

res <- runSimulation(design=Design, replications=500, generate=Generate, 
                     analyse=list(Analyse.MML, Analyse.MCMC), # unnamed list to avoid extra labels
                     summarise=Summarise, fixed_objects=fo,
                     packages='mirt', parallel=TRUE, max_errors = Inf, 
                     filename='KangCohen')
res
```


```{r echo=FALSE}
res <- readRDS("../extras/KangCohen2007/KangCohen.rds")
res
```


# Replication of several Tables and Figures

```{r}
library(dplyr)

# information criteria means/sds
Table6_means <- res %>% filter(model == "1PL") %>%
    select(nitems:sample_size, estimator, means.M1PL.AIC:means.LR23.X2)
Table6_sds <- res %>% filter(model == "1PL") %>%
    select(nitems:sample_size, estimator, sds.M1PL.AIC:sds.LR23.X2)
Table6_means
Table6_sds

Table7_means <- res %>% filter(model == "2PL") %>%
    select(nitems:sample_size, estimator, means.M1PL.AIC:means.LR23.X2)
Table7_sds <- res %>% filter(model == "2PL") %>%
    select(nitems:sample_size, estimator, sds.M1PL.AIC:sds.LR23.X2)
Table7_means
Table7_sds

Table8_means <- res %>% filter(model == "3PL") %>%
    select(nitems:sample_size, estimator, means.M1PL.AIC:means.LR23.X2)
Table8_sds <- res %>% filter(model == "3PL") %>%
    select(nitems:sample_size, estimator, sds.M1PL.AIC:sds.LR23.X2)
Table8_means
Table8_sds

##################################
## proportion of competing models selected (more general measure than nose count)
Table9 <- res %>% filter(estimator == "MML") %>%
    select(nitems:model, M1PL.BIC:M3PL.LR) %>%
    arrange(nitems, sample_size, ability_mean, model)
Table9


##################################

# graphics
library(tidyr)
long <- Table9 %>% 
    pivot_longer(M1PL.BIC:M3PL.LR, names_to='fit_stat') %>% 
    separate(fit_stat, c('fit', 'stat')) %>% 
    mutate(stat=factor(stat), fit=factor(fit), model=factor(model))
long

library(ggplot2)

# unnormalized
# ggplot(long, aes(x=fit, y=value, fill=stat)) +
#     geom_bar(stat = "identity", position = 'dodge') + 
#     facet_grid(model ~ nitems)

# normalized
sub1 <- long %>% group_by(fit, stat, model, nitems) %>% 
    summarise(mvalue = mean(value)) %>% ungroup %>% 
    group_by(model, stat, nitems) %>% mutate(value = mvalue / sum(mvalue)) 
(fig1 <- ggplot(sub1, aes(x=fit, y=value, fill=stat)) +
        geom_bar(stat = "identity", position = 'dodge') + 
        facet_grid(model ~ nitems)) + 
    ggtitle("Figure 1", subtitle = "Model Selection Proportions by Test Length")

# unnormalized
# ggplot(long, aes(x=fit, y=value, fill=stat)) +
#         geom_bar(stat = "identity", position = 'dodge') +
#         facet_grid(model ~ sample_size)

# normalized
sub2 <- long %>% group_by(fit, stat, model, sample_size) %>% 
    summarise(mvalue = mean(value)) %>% ungroup %>% 
    group_by(model, stat, sample_size) %>% mutate(value = mvalue / sum(mvalue)) 
ggplot(sub2, aes(x=fit, y=value, fill=stat)) +
    geom_bar(stat = "identity", position = 'dodge') +
    facet_grid(model ~ sample_size) + 
    ggtitle("Figure 2", subtitle = "Model Selection Proportions by Sample Size")


# unnormalized
# ggplot(long, aes(x=fit, y=value, fill=stat)) +
#         geom_bar(stat = "identity", position = 'dodge') + 
#         facet_grid(model ~ ability_mean)

# normalized
sub3 <- long %>% group_by(fit, stat, model, ability_mean) %>% 
    summarise(mvalue = mean(value)) %>% ungroup %>% 
    group_by(model, stat, ability_mean) %>% mutate(value = mvalue / sum(mvalue)) 
ggplot(sub3, aes(x=fit, y=value, fill=stat)) +
    geom_bar(stat = "identity", position = 'dodge') +
    facet_grid(model ~ ability_mean)  + 
    ggtitle("Figure 3", subtitle = "Model Selection Proportions by Ability Distribution")

```

# Replication comments

The results generally replicate well using the `mirt` software instead of `BILOG-MG`, at least for the 1PL and 2PL models. For the 3PL models, it seems that `mirt` was better able to determine whether the true model was in fact from a population generated with 3PL models when using the likelihood ratio approach, as well as the AIC criteria (see the bottom rows of the above 3 figures in comparison to the original figures in Kang and Cohen, 2007). Similar patterns appear for BIC as well, though it's not entirely clear whether this is due to Monte Carlo sampling error or not (original paper used $R=50$ replication per cell, whereas this simulation used $R=500$). 

In any event, it's unclear why exactly this is the case (perhaps it's due to `mirt`'s logit transformed version of the pseudo-guessing parameter during estimation?) but in any case it seems that correctly selecting the 3PL model using the likelihood-ratio test may not be as poor as the author's originally observed. As such, I'd be more inclined to recommend using the likelihood ratio statistics or AIC when selecting between competing models, 

Note that the above replication does not collapse the $N(-1,1)$ and $N(1,1)$ ability distribution results into a single object as these distributions have very different behaviour in the model comparison results (as they should for the 3PL model, as higher ability individuals would be less influenced by guessing, so the pseudo-guessing parameter would have notably more sampling variability). It's unclear why the authors chose to collapse these categories, but in any event based on the above results alone I would advise against doing so as the marginal results may be misleading. 

