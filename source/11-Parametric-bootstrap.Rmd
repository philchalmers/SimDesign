%\VignetteIndexEntry{SimDesign}
%\VignetteEngine{knitr::knitr}
---
title: "SimDesign"
author: "Phil Chalmers"
date: "September 22, 2015"
output:
  html_document:
    number_sections: yes
    toc: yes
---

```{r include=FALSE}
options(digits = 3)
```

# Parametric boostrap

The parametric boostrap is a type of Monte Carlo simulation method where data is repeatedly generated and analysed based on characteristics from the original data and a specified population of interest. The reason it may be used is to investigate the sampling distirbution of a statistic when no known sampling for exists. In this document we demonstrate how `SimDesign` can be used for parametric boostrapping as well, thereby becoming useful when used within R functions designed for analyses.

## Example

Say that we were intested in an independent t-test type situtation, however we wanted to test whether the medians devide by the inter-quartile range (IQR) would result in a significant effect. As the formula $|M1 - M2| / IQR$ generally has no known sampling distribution we can attempt to simulate one instead if we can make some reasonable assumptions about the population. For instance, say that the null distribution implies that $H_0: |M1 - M2| = 0$, and that the population distribution is normally distributed. 

```{r include=FALSE}
set.seed(1)
```


```{r}
median_diff <- function(DV, group){
    meds <- tapply(DV, group, median) 
    IQR <- IQR(DV)
    unname(abs(meds[1] - meds[2]) / IQR)
}
dat <- data.frame(DV = rnorm(100), group = rep(c('Group 1', 'Group 2'), each = 50))
with(dat, median_diff(DV, group))
```

The question now is: given the observed statistic, is there evidence indicating that `r with(dat, median_diff(DV, group))` is significantly greater than 0? Using the tools from `SimDesign` we could construct a parametric boostrap method to tests this under the assumption that the population came from a standard normal distribution.

```{r}
median_parametric_boot <- function(dat, fun, R = 1000, verbose = FALSE, ...){
    
    observed <- with(dat, fun(DV, group))
    
    require(SimDesign)
    Design <- data.frame(N=nrow(dat))
    fixed_objects <- list(fun = fun) #export fun to make it available across nodes
    
    Generate <- function(condition, fixed_objects = NULL) {
        dat <- data.frame(DV = rnorm(condition$N), 
                          group = rep(c('Group 1', 'Group 2'), each = condition$N/2))
        dat
    }

    Analyse <- function(condition, dat, fixed_objects = NULL, parameters = NULL) {
        ret <- with(dat, fixed_objects$fun(DV, group))
        ret
    }

    # no summarise function provided because design only contains 1 row. Returns a matrix
    results <- runSimulation(design=Design, replications=R, generate=Generate, analyse=Analyse, 
                             fixed_objects=fixed_objects, verbose=verbose, ...)
    
    # empirical p-value
    ret <- c(p = colMeans(observed < results))
    ret
}
```

Using this function definition we can pass the necessary elements to obtain a parameterically estimate $p$-value indicating how likely the observed statistic was to observe given that a suitable emperically generated null hypothesis distribution. 

```{r}
median_parametric_boot(dat, median_diff)
```

Furthermore, because additional arguments can be passed through the `...` input to `runSimulation` all the extra components available in the package will be present. E.g., defining a parallel cluster and running the code in parallel:

```{r}
cl <- parallel::makeCluster(2)
median_parametric_boot(dat, median_diff, cl=cl, parallel=TRUE)
```

```{r include=FALSE}
parallel::stopCluster(cl)
```


