---
title: "Power-curve estimates"
author: "Phil Chalmers"
format:
  html:
    theme:
      dark: darkly
      light: spacelab
    number_sections: true
    toc: true
    fontsize: 1.1em
    embed-resources: true
---

```{r include=FALSE}
options(digits = 3)
set.seed(1)
```

Power-curves are useful when researchers are interested in the range of power effects when varying different population and sample properties. For instance, researchers may be interested in how power improves as the magnitude of the population mean difference for an independent samples $t$-test increases, or how quickly power improves when increasing the sample size ($N$). The following simulation assumes that the population variances are equal.   

### Define the functions

```{r}
library(SimDesign)
#SimFunctions(comments = FALSE)

### Define design conditions. Here we want a finer range of values for plotting
Design <- createDesign(mean_diff = seq(0, 1, by = .1), 
                       sample_size = c(10, 20, 30))

#--------------------------------------------------------------------

Generate <- function(condition, fixed_objects = NULL) {
  Attach(condition)
  dat1 <- rnorm(sample_size)
  dat2 <- rnorm(sample_size, mean=mean_diff)
  dat <- list(dat1=dat1, dat2=dat2)
  dat
}

Analyse <- function(condition, dat, fixed_objects = NULL) {
  ret <- c(p = t.test(dat$dat1, dat$dat2, 
                      var.equal=TRUE)$p.value)
  ret
}

Summarise <- function(condition, results, fixed_objects = NULL) {
  ret <- EDR(results, alpha = .05)
  ret
}

#--------------------------------------------------------------------

### Run the simulation (using all available cores)
res <- runSimulation(Design, replications=10000, verbose=FALSE, parallel=TRUE,
                     generate=Generate, analyse=Analyse, summarise=Summarise)
```

```{r}
# summary of simulation object
summary(res)

# print results
print(res)
```

A power curve is created by placing the detection rates on a $y$-axis and including different factors on the x-axis and other aesthetics (shading, colours, etc). Here we use `ggplot2` to construct suitable power-curves.

```{r fig.align='center'}
library(ggplot2)
ggplot(res, aes(mean_diff, p, colour=factor(sample_size))) + geom_point() + geom_line() +
    xlab('Mean Difference') + ylab('Detection Rate') + ggtitle('Empirical Power Curves') +
    scale_color_discrete('N')
```

Compare these results to analytic formulas, we can see that the two are essentially coinciding with good accuracy. 

```{r}
library(pwr)
pwr.t.test(d=0.5, n=30, sig.level=0.05, type="two.sample", alternative="two.sided")
subset(res, mean_diff == .5 & sample_size == 30)
pwr.t.test(d=0.5, n=10, sig.level=0.05, type="two.sample", alternative="two.sided")
subset(res, mean_diff == .5 & sample_size == 10)
```

# Power-curve with Bootstrapped Confidence Intervals

In situations where analytic power estimation is not available, and therefore simulations are used as a suitable stand-in, it's important to evaluate the *precision* of the observed power estimates from the simulation study. As such, one reasonable (and extremely general approach) is to apply bootstrapping methodology to the within replication; for simulated power analyses this involves bootstrapping the $R$ replicated $p$-values to obtain suitable non-parametric confidence intervals. 

The following implements the empirical (i.e., basic) bootstrap CI method for the simulation above, and instead plots the point-wise 95% CIs estimates for each condition in the `Design` input. Notice that the replications were reduced from 10000 to 1000 to demonstrate the potential uncertainty when using too few replications when estimating power via simulation.

```{r}
### Run the simulation (using all available cores)
res <- runSimulation(Design, replications=1000, verbose=FALSE, parallel=TRUE,
                     generate=Generate, analyse=Analyse, summarise=Summarise,
                     boot_method = 'basic')
res
```

```{r fig.align='center'}
library(ggplot2)
ggplot(res, aes(mean_diff, p, colour=factor(sample_size))) + geom_point() + geom_line() +
    geom_ribbon(aes(ymin=BOOT_p_2.5, ymax=BOOT_p_97.5, fill=factor(sample_size)), alpha=.3, linetype=0) + 
    xlab('Mean Difference') + ylab('Detection Rate') + ggtitle('Empirical Power Curves with 95% CIs') +
    scale_fill_discrete('N') + scale_colour_discrete('N')
```

Finally, when increasing the replications back to 10000 it become visually clear how precise the original point-estimates  presented in the previous section were. 

```{r}
### Run the simulation (using all available cores)
res <- runSimulation(Design, replications=10000, verbose=FALSE, parallel=TRUE,
                     generate=Generate, analyse=Analyse, summarise=Summarise,
                     boot_method = 'basic')
res
```

```{r fig.align='center'}
library(ggplot2)
ggplot(res, aes(mean_diff, p, colour=factor(sample_size))) + geom_point() + geom_line() +
    geom_ribbon(aes(ymin=BOOT_p_2.5, ymax=BOOT_p_97.5, fill=factor(sample_size)), alpha=.3, linetype=0) + 
    xlab('Mean Difference') + ylab('Detection Rate') + ggtitle('Empirical Power Curves with 95% CIs') +
    scale_fill_discrete('N') + scale_colour_discrete('N')
```


# Power-curve for Solving Sample Size via Interpolations

Power-curves can also be useful for obtaining estimates to satisfy particular power rates when sample size planning. Note that this is a rather inefficient approach to solving sample sizes, though it has been very popular (see `?SimDesign::SimSolve` for a stochastic root-solving alternative). 

In the following, a $d=.5$ effect size is assumed in the population, where the value of $N$ associated with a target power rate of $.80$ is desired. To obtain an $\hat{N}$ that is roughly associated with this target power a range of $N$ inputs are first evaluated, the adjacent points closest to .80 are isolated, and finally the $\hat{N}$ is solved using linear interpolations to obtain an approximate target sample size.

```{r}
Design <- createDesign(mean_diff = .5, # equivalent to Cohen's "medium" d effect size
                       sample_size = seq(10, 100, by=5))

res <- runSimulation(Design, replications=10000, verbose=FALSE, parallel=TRUE,
                     generate=Generate, analyse=Analyse, summarise=Summarise)

# target N somewhere between 60 and 65
res

# solve using linear interpolated power
pmin <- min(which(res$p > .80))
pick <- res[c(pmin-1, pmin),]
pick
```

After the empirical power curve has been built, and it's clear the target power is within the range specified when building the power curve, interpolations can be used to solve for $\hat{N}$. In this case a linear interpolation is used between the closest points, though alternatives are of course possible (e.g., fitting higher-order polynomials, splines, or other non-linear models).

```{r}
mod <- lm(p ~ sample_size, pick)
coef(mod)
fn <- function(N, power = .8)
	predict(mod, newdata = data.frame(sample_size=N)) - power
root <- uniroot(fn, interval = c(10, 100))
root  # sample size estimate from the root solver

# build power curve and include interpolation estimates
ggplot(res, aes(sample_size, p)) + geom_point() + geom_line() + 
    xlab('N') + ylab('Detection Rate') + 
	geom_abline(intercept = .80, slope=0, col='red', linetype='dashed') +
	geom_vline(xintercept=root$root, col='blue', linetype='dashed') + 
	annotate("text", x=80, y=.6, label = paste0('N estimate = ', round(root$root, 2))) + 
	ggtitle('Power curve with linear interpolations')
```

Finally, in this case one can compare the $\hat{N}$ with the $N$ associated with a power of .80 given the well-known non-central $t$-distribution using the `pwr` package. Had the associated non-central distribution been unavailable further Monte Carlo simulations around the estimate would be required (e.g., using `runSimulation()` given the integer values for `sample_size` given `r floor(root$root)` and `r ceiling(root$root)`). 

```{r}
# true N from root-solving the deterministic power function
pwr.t.test(d=0.5, sig.level=0.05, power=.8, type="two.sample", alternative="two.sided")
```

