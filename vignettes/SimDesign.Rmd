%\VignetteIndexEntry{simTool}
%\VignetteEngine{knitr::knitr}
---
title: "SimDesign"
author: "Phil Chalmers"
date: "September 22, 2015"
output:
  html_document:
    number_sections: yes
    toc: yes
---

# Introduction


Intro stub describing general workflow.


```{r}
library(SimDesign)
```

To begin defining the required function, a quick call to `SimDesign_functions()` will create a template to be
filled in with all the necessary functional arguments pre-assigned. This should be copied and saved to an external source file.

```{r}
SimDesign_functions()
```

### Debugging and editing

Something

# Simulation 1: Type I error rates and Power rates

Describe simulation scenerio TODO

## Define conditions

First, define the condition combinations that should be investigated. In this case we know that the size and ratio of the sample sizes within each group, as well as the respective group standard deviations, will influence the detection rates of the independent t-test. Here we study define three different sample sizes and standard deviations, and investigate their effect when completely crossed.

```{r conditions}
sample_sizes <- c(10, 20, 50, 100)
standard_deviation_ratios <- c(1, 4, 8)
mean_difference <- c(0, 0.5)

Design <- expand.grid(sample_size_group1=sample_sizes,
                      sample_size_group2=sample_sizes,
                      standard_deviation_ratio=standard_deviation_ratios,
                      mean_difference=mean_difference)
dim(Design)
head(Design, 3)
tail(Design, 3)
```

Each row in `Design` represents a unique condition to be studied in the simulation. In this case, the first condition to be studied comes from row 1, where both broups have a sample size of 10 and the ratio of the standard devitions are 1 (i.e., they are exactly equal) and there is no mean difference between the groups. Analysing rows with no mean difference will give us an idea of the Type I error rates. The last condition on row 96 indicates that both groups have sample sizes of 100 but that one of the standard deviations is 8 times larger than the other and a mean difference of 0.5. Which group is larger in both the standard deviation and mean will be defined in the `Generate()` function. 

## Define the functions

We first start by defining the `Generate()` function. The only argument accepted by this function is `condition`, which will always be a *single row from the Design data.frame object*. Conditions are run sequentially from row 1 to the last row in `Design`.

```{r}
Generate <- function(condition){
 
    N1 <- condition$sample_size_group1
    N2 <- condition$sample_size_group2
    sd <- condition$standard_deviation_ratio
    md <- condition$mean_difference
    
    group1 <- rnorm(N1)
    group2 <- rnorm(N2, mean=md, sd=sd)
    dat <- data.frame(group = c(rep('g1', N1), rep('g2', N2)), DV = c(group1, group2))
    
    return(list(dat=dat, parameters=list()))
}
```

The `Generate()` function defined above first extracts the elements from `condition` for less verbose
ease of use, generates data for two groups using random normal draws, and returns a data.frame object 
containing the dependent variable and grouping information. 

```{r}
Analyse <- function(dat, parameters, condition){
    
    require(stats)

    welch <- try(t.test(DV ~ group, dat), silent=TRUE)
    ind <- try(t.test(DV ~ group, dat, var.equal=TRUE), silent=TRUE)
    check_error(welch, ind)

    ret <- c(welch=welch$p.value, independent=ind$p.value)
    return(ret)
}
```

The `Analyse()` defintion performs a Welch and independent t-test on the data generated from `Generate()`, checks whether any errors occured during the calls to the estimation functions, and extracts and returns the respective p-values for each statistical analysis.

Finally, after the number of replications defined in `replications` has been completed a matrix (or list, if the output of `Analyse()` were a list instead of a vector) the overall results should be summerised into suitable meta-descriptions of the behavior. This is accomplished with the `Summerise()` function.

```{r}
Summarise <- function(results, parameters_list, condition){
    
    nms <- c('welch', 'independent')
    lessthan.05 <- EDR(results[,nms], alpha = .05)
    
    ret <- c(lessthan.05=lessthan.05)
    return(ret)
}
```

In the above function, the `results` input is a matrix of size `replications` by 2 containing the respective 
p-values returned from `Analyse()` across the desired number of replications. It is here where empirical 
detection rates can be determined, and a convinience function `EDR()` is avaialbe for just this purpose. Each column in the supplied matrix of p-values is checked to see whether it is less that $\alpha = .05$, and the
average number of times this occurs is returned as a vector of length 2. Names are preserved in the `EDR()` 
function, therefore it is clear which element represents the Welch and Independent t-test rates.


## Run the simulation

```{r cache=TRUE, eval=TRUE}
Final <- runSimulation(design = Design, replications = 1000, verbose = TRUE, parallel = FALSE,
                       generate = Generate, analyse = Analyse, summarise = Summarise)
```

## Analyse the results

Usually it is a good idea to split up analyses into Type I error and power data. More specifically, power rates really should only be interpreted when the Type I errors are well behaved (liberal detection rates naturally will result in more powerful tests, while conservative detection rates will induce lower power).


```{r eval=FALSE}
TypeI <- subset(Final, mean_difference == 0)
Power <- subset(Final, mean_difference != 0)
```

# Simulation 2: Determine estimator efficiency


## Define conditions

Stuff


# Simulation 3: Comparing two estimators

Description

## Define conditions

```{r}
sample_sizes <- c(100, 250, 500, 1000)
test_length <- c(10, 20, 30)
Design <- expand.grid(sample_size=sample_sizes, test_length=test_length)

# save extra verbose information in an external file, and read it in later
set.seed(1234)
aux_info <- vector('list', 3)
names(aux_info) <- test_length
for(i in 1:3)
    aux_info[[i]] <- list(a = round(rlnorm(test_length[i], .2, .3), 2), 
                          d = round(rnorm(test_length[i], 0, .5), 2))
aux_info
saveRDS(aux_info, 'parameters.rds')
```

## Define the functions

Something...

```{r}
Generate <- function(condition) {
    
    library(mirt) #for simdata() function
    nitems <- as.character(condition$test_length)
    N <- condition$sample_size
        
    #source in
    parameters <- readRDS('parameters.rds')
    a <- matrix(parameters[[nitems]]$a)
    d <- matrix(parameters[[nitems]]$d)
    
    dat <- as.data.frame(simdata(a=a, d=d, N=N, itemtype = 'dich'))
    parameters <- list(a=a, d=d)

    return(list(dat=dat, parameters=parameters))
}

Analyse <- function(dat, parameters, condition) {
    
    library(mirt)
    library(lavaan)
    nitems <- condition$test_length
    
    mod <- try(mirt(dat, 1L, verbose=FALSE), silent=TRUE)
    if(mod@converge != 1) stop('did not converge')
    cfs <- coef(mod, simplify = TRUE, digits = Inf)
    FIML_as <- cfs$items[,1L]
    FIML_ds <- cfs$items[,2L]
    
    lavmod <- paste0('F =~ ', paste0('NA*', colnames(dat)[1L], ' + '), 
                     paste0(colnames(dat)[-1L], collapse = ' + '),
                     '\nF ~~ 1*F')
    lmod <- try(sem(lavmod, dat, ordered = colnames(dat)))
    cfs2 <- coef(lmod) * 1.702 # scaling adjustment
    DWLS_as <- cfs2[1L:nitems]
    DWLS_ds <- -1 * cfs2[(1L:nitems) + nitems]

    return(list(FIML_as=FIML_as, FIML_ds=FIML_ds, DWLS_as=DWLS_as, DWLS_ds=DWLS_ds))
}

Summarise <- function(results, parameters_list, condition) {
    
    browser()
    pop_as <- something
    pop_ds <- something
    index <- 1:condition$test_length
    
    FIML_as <- do.call(rbind, lapply(results, function(x) x[['FIML_as']]))
    FIML_ds <- do.call(rbind, lapply(results, function(x) x[['FIML_ds']]))
    DWLS_as <- do.call(rbind, lapply(results, function(x) x[['DWLS_as']]))
    DWLS_ds <- do.call(rbind, lapply(results, function(x) x[['DWLS_ds']]))
    
    bias_FIML_as <- sapply(index, function(ind, obs, pop) bias(obs[,ind], pop[ind]),
                           obs=FIML_as, pop=pop_as)
    bias_DWLS_as <- sapply(index, function(ind, obs, pop) bias(obs[,ind], pop[ind]),
                           obs=DWLS_as, pop=pop_as)
    bias_FIML_ds <- sapply(index, function(ind, obs, pop) bias(obs[,ind], pop[ind]),
                           obs=FIML_ds, pop=pop_ds)
    bias_DWLS_ds <- sapply(index, function(ind, obs, pop) bias(obs[,ind], pop[ind]),
                           obs=DWLS_ds, pop=pop_ds)
    RMSE_FIML_as <- sapply(index, function(ind, obs, pop) RMSE(obs[,ind], pop[ind]),
                           obs=FIML_as, pop=pop_as)
    RMSE_DWLS_as <- sapply(index, function(ind, obs, pop) RMSE(obs[,ind], pop[ind]),
                           obs=DWLS_as, pop=pop_as)
    RMSE_FIML_ds <- sapply(index, function(ind, obs, pop) RMSE(obs[,ind], pop[ind]),
                           obs=FIML_ds, pop=pop_ds)
    RMSE_DWLS_ds <- sapply(index, function(ind, obs, pop) RMSE(obs[,ind], pop[ind]),
                           obs=DWLS_ds, pop=pop_ds)
    
    ret <- c(bias_FIML_as=bias_FIML_as, bias_DWLS_as=bias_DWLS_as, 
             bias_FIML_ds=bias_FIML_ds, bias_DWLS_ds=bias_DWLS_ds,
             RMSE_FIML_as=RMSE_FIML_as, RMSE_DWLS_as=RMSE_DWLS_as,
             RMSE_FIML_ds=RMSE_FIML_ds, RMSE_DWLS_ds=RMSE_DWLS_ds)
    return(ret)
}
```


```{r cache=TRUE, eval=FALSE}
res10 <- runSimulation(subset(Design, test_length == 10), replications = 1000, 
                       generate=Generate, analyse=Analyse, summarise=Summarise, parallel = TRUE)
res20 <- runSimulation(subset(Design, test_length == 20), replications = 1000, 
                       generate=Generate, analyse=Analyse, summarise=Summarise, parallel = TRUE)
res30 <- runSimulation(subset(Design, test_length == 30), replications = 1000, 
                       generate=Generate, analyse=Analyse, summarise=Summarise, parallel = TRUE)
```
